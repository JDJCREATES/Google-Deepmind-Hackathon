"""
LangGraph nodes for hypothesis market.

Each node represents a step in the hypothesis-driven reasoning process:
1. Load knowledge
2. Classify frameworks
3. Generate hypotheses
4. Gather evidence
5. Update beliefs
6. Score decisions
7. Select/execute action
8. Counterfactual replay
9. Check drift
10. Evolve policy (if needed)
"""
from __future__ import annotations

from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple
from uuid import uuid4

from pydantic import BaseModel, Field
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage

from app.config import settings
from app.hypothesis import (
    BeliefState,
    Evidence,
    Hypothesis,
    HypothesisFramework,
    create_hypothesis,
)
from app.knowledge import get_knowledge_base
from app.reasoning import (
    CounterfactualReplay,
    DecisionPolicy,
    FrameworkDriftDetector,
    StrategicMemory,
)
from app.utils.logging import get_agent_logger
from app.utils.llm import with_retry
from app.graphs.state import HypothesisMarketState


def extract_gemini_content(content: Any) -> Tuple[str, Optional[str]]:
    """
    Extract text and thinking from Gemini 3 response content.
    
    When include_thoughts=True, content is a list:
    [{'type': 'thinking', 'thinking': '...'}, {'type': 'text', 'text': '...', 'extras': {...}}]
    
    Returns:
        (text_content, thinking_content) - thinking may be None
    """
    if isinstance(content, str):
        return content, None
    
    if isinstance(content, list):
        text = ""
        thinking = None
        for part in content:
            if isinstance(part, dict):
                if part.get('type') == 'thinking':
                    thinking = part.get('thinking', '')
                elif part.get('type') == 'text':
                    text = part.get('text', '')
        return text, thinking
    
    return str(content), None


logger = get_agent_logger("HypothesisNodes")

# Shared instances
drift_detector = FrameworkDriftDetector()
# strategic_memory is now imported as a persistent singleton
from app.reasoning.counterfactual import strategic_memory

# ===========================================
# Agent Singletons (Performance Optimization)
# ===========================================
# Cache agents to avoid expensive re-initialization on every hypothesis cycle.
# Each agent init creates SQLite connections and potentially Gemini context caches.

_agent_cache = {}

def get_cached_agent(agent_class):
    """Get or create a cached singleton instance of an agent."""
    class_name = agent_class.__name__
    if class_name not in _agent_cache:
        _agent_cache[class_name] = agent_class()
    return _agent_cache[class_name]

def clear_agent_cache():
    """Clear agent cache (call on simulation stop/reset)."""
    global _agent_cache
    _agent_cache = {}


# ==========================================
# Structured Output Models (Pydantic)
# ==========================================

class FrameworkClassification(BaseModel):
    """Result of framework classification."""
    frameworks: List[str] = Field(
        description="List of applicable epistemic frameworks (e.g. ['RCA', 'TOC'])",
        min_items=1
    )
    reasoning: str = Field(description="Brief explanation of why these frameworks apply")

class BeliefUpdateResult(BaseModel):
    """Result of Bayesian belief update."""
    posteriors: Dict[str, float] = Field(
        description="Map of hypothesis IDs to posterior probabilities"
    )
    leading_hypothesis_id: Optional[str] = Field(
        description="ID of the hypothesis with highest posterior",
        default=None
    )
    leader_confidence: float = Field(
        description="Posterior probability of the leading hypothesis",
        ge=0.0,
        le=1.0
    )
    converged: bool = Field(
        description="Whether confidence is high enough to act (>0.7)",
        default=False
    )
    reasoning: str = Field(description="Explanation of the belief update calculation")


async def load_knowledge_node(state: HypothesisMarketState) -> Dict[str, Any]:
    """
    Load relevant company knowledge for the signal.
    
    Retrieves policies, procedures, and SOPs relevant to the
    current signal type to ground Gemini's reasoning.
    """
    logger.info(f"ðŸ“š Loading knowledge for signal: {state['signal_type']}")
    
    kb = get_knowledge_base()
    context = kb.get_context_for_signal(
        state["signal_type"],
        keywords=list(state.get("signal_data", {}).keys())
    )
    
    return {"knowledge_context": context}


async def classify_frameworks_node(state: HypothesisMarketState) -> Dict[str, Any]:
    """
    Classify which epistemic frameworks apply to this signal.
    
    Uses Gemini to determine which of the 5 frameworks
    (RCA, COUNTERFACTUAL, FMEA, TOC, HACCP) are relevant.
    """
    logger.info(f"ðŸ·ï¸ Classifying frameworks for: {state['signal_description']}")
    
    llm = ChatGoogleGenerativeAI(
        model=settings.gemini_model,
        google_api_key=settings.google_api_key,
        temperature=0.3,
    )
    
    # We need raw access for thought signatures, so we don't use with_structured_output directly on the LLM
    # Instead we use a parser
    from langchain_core.output_parsers import PydanticOutputParser
    from langchain_core.prompts import ChatPromptTemplate
    
    parser = PydanticOutputParser(pydantic_object=FrameworkClassification)
    
    prompt_text = f"""
Classify which epistemic frameworks apply to this manufacturing signal.

SIGNAL: {state['signal_description']}
DATA: {state['signal_data']}

AVAILABLE FRAMEWORKS:
1. RCA (Root Cause Analysis) - For "Why is this happening?" questions
2. COUNTERFACTUAL - For "What if we act/don't act?" analysis
3. FMEA (Failure Mode Effects Analysis) - For "What could go wrong?" risks
4. TOC (Theory of Constraints) - For "What's the bottleneck?" optimization
5. HACCP - For "Are we violating compliance?" food safety

Select at least 2 relevant frameworks.

{parser.get_format_instructions()}
"""
    
    messages = [HumanMessage(content=prompt_text)]
    
    # Note: Thought signature continuity is now handled via include_thoughts=True
    # The old manual injection caused MESSAGE_COERCION_FAILURE

    try:
        # Raw invoke to get message with potential thought signature
        msg = await llm.ainvoke(messages)
        
        # Extract text and thinking from Gemini 3 response
        text_content, thinking = extract_gemini_content(msg.content)
        
        # Broadcast thinking if available
        if thinking:
            from app.services.websocket import manager
            await manager.broadcast({
                "type": "agent_thinking",
                "data": {
                    "agent": "hypothesis_market",
                    "thought": thinking[:300],
                    "timestamp": datetime.now().isoformat()
                }
            })
        
        # Parse content
        # Strip markdown if present
        if "```json" in text_content:
            text_content = text_content.split("```json")[1].split("```")[0].strip()
        elif "```" in text_content:
            text_content = text_content.split("```")[1].split("```")[0].strip()

        result = parser.parse(text_content)
        frameworks = result.frameworks
        reasoning = result.reasoning
        
        # 2. Extract Thought Signature
        new_sig = None
        if hasattr(msg, 'content') and isinstance(msg.content, list):
             for part in msg.content:
                 if isinstance(part, dict) and 'thought_signature' in part:
                     new_sig = part['thought_signature']
                     break
        elif hasattr(msg, 'content') and isinstance(msg.content, dict) and 'thought_signature' in msg.content:
             new_sig = msg.content['thought_signature']
             
        if new_sig:
            logger.debug("Captured thought signature from classify_frameworks")
        
    except Exception as e:
        logger.error(f"Failed to classify frameworks: {e}")
        frameworks = ["RCA", "TOC"]  # Fallback
        new_sig = state.get("thought_signature")  # Preserve old if failed
    
    # Store in signal_data for downstream use
    updated_data = dict(state.get("signal_data", {}))
    updated_data["applicable_frameworks"] = frameworks
    
    return {
        "signal_data": updated_data,
        "thought_signature": new_sig
    }


async def generate_hypotheses_node(state: HypothesisMarketState) -> Dict[str, Any]:
    """
    Generate competing hypotheses by querying specialized agents.
    
    Instead of a single LLM call, this node acts as a market maker,
    soliciting hypotheses from Production, Staffing, Compliance, 
    and Maintenance agents based on their domain expertise.
    """
    logger.info("ðŸ’¡ Generating hypotheses via distributed agent market")
    
    # Broadcast start of hypothesis generation
    from app.services.websocket import manager
    
    # 1. Orchestrator Announcement
    await manager.broadcast({
        "type": "agent_action",
        "data": {
            "agent": "MasterOrchestrator",
            "actions": ["Convening specialist agents for hypothesis generation..."],
            "timestamp": datetime.now().isoformat()
        }
    })

    await manager.broadcast({
        "type": "reasoning_phase",
        "data": {
            "phase": "hypothesis_generation",
            "signal": state["signal_description"],
            "timestamp": datetime.now().isoformat()
        }
    })
    
    # Import agents here to avoid circular imports at module level
    from app.agents.production.production_agent import ProductionAgent
    from app.agents.staffing.staffing_agent import StaffingAgent
    from app.agents.compliance.compliance_agent import ComplianceAgent
    from app.agents.maintenance.maintenance_agent import MaintenanceAgent
    from app.agents.orchestrator.orchestrator import MasterOrchestrator
    
    # Context for agents
    signal = {
        "description": state["signal_description"],
        "data": state.get("signal_data", {}),
        "knowledge": state.get("knowledge_context", "")
    }
    
    hypotheses = []
    
    # 1. Get cached agent instances (PERFORMANCE: Avoids re-initialization overhead)
    # Previously created new instances each cycle, costing ~2-3s latency per agent
    agents = [
        get_cached_agent(ProductionAgent),
        get_cached_agent(StaffingAgent),
        get_cached_agent(ComplianceAgent),
        get_cached_agent(MaintenanceAgent)
    ]
    
    # 2. Solicit hypotheses from all agents in parallel
    import asyncio
    
    tasks = []
    for agent in agents:
        # Optimization: Filter context to reduce token usage per agent
        # We pass the full signal data to the filter
        filtered_data = agent.filter_context(signal["data"])
        
        # Create agent-specific signal copy with filtered data
        agent_signal = signal.copy()
        agent_signal["data"] = filtered_data
        
        tasks.append(agent.generate_hypotheses(agent_signal))

    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    for result in results:
        if isinstance(result, list):
            hypotheses.extend(result)
        elif isinstance(result, Exception):
            logger.error(f"Agent failed to generate hypotheses: {result}")

    # 3. Master Orchestrator creates a synthesis/counterfactual hypothesis
    # if specific frameworks are requested (e.g. Counterfactual)
    if "COUNTERFACTUAL" in state.get("signal_data", {}).get("applicable_frameworks", []):
        orch = MasterOrchestrator()
        orch_hypos = await orch.generate_hypotheses(signal)
        hypotheses.extend(orch_hypos)
    
    # Checks for empty results
    if not hypotheses:
        # Fallback to generic generation if agents return nothing
        logger.warning("No hypotheses from agents - falling back to generic generation")
        from app.hypothesis import create_hypothesis
        hypotheses.append(create_hypothesis(
            framework="RCA",
            description=f"Generic investigation needed for {signal['description']}",
            initial_confidence=0.3,
            proposed_by="SystemFallback"
        ))
        
    logger.info(f"âœ… Aggregated {len(hypotheses)} hypotheses from agents")
    
    # Broadcast generated hypotheses for frontend visualization
    await manager.broadcast({
        "type": "hypotheses_generated",
        "data": {
            "count": len(hypotheses),
            "hypotheses": [
                {
                    "id": h.hypothesis_id,
                    "description": h.description,
                    "confidence": h.initial_confidence,
                    "proposed_by": h.proposed_by,
                    "cost_if_wrong": h.reversibility
                }
                for h in hypotheses
            ],
            "timestamp": datetime.now().isoformat()
        }
    })
    
    return {"hypotheses": hypotheses}


async def gather_evidence_node(state: HypothesisMarketState) -> Dict[str, Any]:
    """
    Gather evidence for each hypothesis using dynamic agent tool proposals.
    
    Now parallelized for performance.
    """
    logger.info("ðŸ” Gathering evidence via dynamic agent tooling")
    
    # Import agents (lazy import to avoid circular dep)
    from app.agents.production.production_agent import ProductionAgent
    from app.agents.staffing.staffing_agent import StaffingAgent
    from app.agents.compliance.compliance_agent import ComplianceAgent
    from app.agents.maintenance.maintenance_agent import MaintenanceAgent
    from app.agents.orchestrator.orchestrator import MasterOrchestrator
    
    # Agent map (PERFORMANCE: Use cached singletons)
    agent_map = {
        "ProductionAgent": get_cached_agent(ProductionAgent),
        "ComplianceAgent": get_cached_agent(ComplianceAgent),
        "MaintenanceAgent": get_cached_agent(MaintenanceAgent),
        "StaffingAgent": get_cached_agent(StaffingAgent),
        "MasterOrchestrator": get_cached_agent(MasterOrchestrator),
    }
    
    hypotheses = state.get("hypotheses", [])
    evidence_list = []
    
    # Simulate tool execution (Mock Engine)
    def simulate_tool_output(tool_name: str, params: Dict, hypothesis_desc: str) -> Dict[str, Any]:
        """Simulate realistic tool output based on the tool and hypothesis context."""
        import random
        
        # Is this likely true? Bias towards the first hypothesis being true for simulation
        is_supported = "smoke" in hypothesis_desc.lower() or "fire" in hypothesis_desc.lower() or random.random() > 0.6
        strength = 0.8 + (random.random() * 0.15) if is_supported else 0.2
        
        if "check_sensors" in tool_name:
            return {
                "reading": 85.4 if is_supported else 24.1, 
                "threshold": 50.0, 
                "status": "CRITICAL" if is_supported else "NORMAL",
                "supports": is_supported
            }
        elif "camera" in tool_name:
            return {
                "visual_anomaly_detected": is_supported,
                "confidence": 0.92,
                "description": "Visible smoke plume" if is_supported else "Clear view",
                "supports": is_supported
            }
        elif "logs" in tool_name:
            return {
                "events_found": 12 if is_supported else 0,
                "pattern_match": is_supported,
                "supports": is_supported
            }
            
        return {"result": f"Simulated check for {tool_name}", "supports": is_supported}

    
    # Define async verification task
    async def verify_hypothesis(hypothesis):
        # Determine responsible agent
        agent_name = hypothesis.proposed_by or "ProductionAgent"
        agent = agent_map.get(agent_name, agent_map["ProductionAgent"])
        
        # 1. Ask agent to propose verification (SILENTLY)
        # We broadcast a managed Orchestrator log instead
        verification_plan = await agent.propose_verification(hypothesis, silence=True)
        
        # Managed log broadcast
        from app.services.websocket import manager
        await manager.broadcast({
            "type": "agent_action",
            "data": {
                "agent": agent_name,
                "actions": [f"Verifying hypothesis: {verification_plan.get('reasoning', 'Standard check')[:100]}..."],
                "timestamp": datetime.now().isoformat()
            }
        })
        
        tool_name = verification_plan.get("tool", "manual_check")
        rationale = verification_plan.get("reasoning", "Standard verification")
        params = verification_plan.get("params", {})
        
        # 2. "Execute" the tool (Simulated)
        sim_result = simulate_tool_output(tool_name, params, hypothesis.description)
        
        # 3. Create Evidence object
        evidence = Evidence(
            source=f"{tool_name}",
            data={
                "tool": tool_name,
                "params": params,
                "raw_output": sim_result
            },
            supports=sim_result["supports"],
            strength=0.85 if sim_result["supports"] else 0.4,
            gathered_by=agent_name
        )
        
        # Broadcast tool usage for frontend
        from app.services.websocket import manager
        await manager.broadcast({
            "type": "tool_execution",
            "data": {
                "agent": agent_name,
                "tool": tool_name,
                "rationale": rationale,
                "result": sim_result,
                "timestamp": datetime.now().isoformat()
            }
        })
        
        return evidence

    # Execute all verifications in parallel
    import asyncio
    verification_tasks = [verify_hypothesis(h) for h in hypotheses]
    gathered_evidence = await asyncio.gather(*verification_tasks, return_exceptions=True)
    
    # Process results
    for res in gathered_evidence:
        if isinstance(res, Evidence):
            evidence_list.append(res)
        elif isinstance(res, Exception):
            logger.error(f"Evidence gathering failed: {res}")
            
    logger.info(f"âœ… Gathered {len(evidence_list)} pieces of dynamic evidence")
    
    return {"evidence": evidence_list}


async def update_beliefs_node(state: HypothesisMarketState) -> Dict[str, Any]:
    """
    Update beliefs using Bayesian reasoning via Gemini.
    
    Analyzes evidence impact on each hypothesis and computes
    posterior probabilities.
    """
    logger.info("ðŸ§® Updating beliefs")
    
    hypotheses = state.get("hypotheses", [])
    evidence = state.get("evidence", [])
    
    if not hypotheses:
        return {"converged": False}
    
    llm = ChatGoogleGenerativeAI(
        model=settings.gemini_model,
        google_api_key=settings.google_api_key,
        temperature=0.2,  # Low for precise reasoning
    )
    
    from langchain_core.output_parsers import PydanticOutputParser
    parser = PydanticOutputParser(pydantic_object=BeliefUpdateResult)
    
    prompt_text = f"""
Perform Bayesian belief update for these hypotheses given the evidence.

HYPOTHESES:
{[{"id": h.hypothesis_id, "desc": h.description, "prior": h.initial_confidence} for h in hypotheses]}

EVIDENCE:
{[{"source": e.source, "data": e.data, "supports": e.supports, "strength": e.strength} for e in evidence]}

For each hypothesis:
1. Analyze how evidence affects probability
2. Calculate posterior probability
3. Ensure posteriors sum to approximately 1.0

Output the detailed calculations and final posteriors according to the schema.

{parser.get_format_instructions()}
"""

    messages = [HumanMessage(content=prompt_text)]
    
    # Note: Thought signature continuity now handled via include_thoughts=True

    try:
        msg = await llm.ainvoke(messages)
        
        # 1. Extract text and thinking from Gemini 3 response
        text_content, thinking = extract_gemini_content(msg.content)
        
        # Broadcast thinking if available
        if thinking:
            from app.services.websocket import manager
            await manager.broadcast({
                "type": "agent_thinking",
                "data": {
                    "agent": "hypothesis_market",
                    "thought": thinking[:300],
                    "timestamp": datetime.now().isoformat()
                }
            })
        
        # 2. Parse JSON content
        # Strip markdown if present
        if "```json" in text_content:
            text_content = text_content.split("```json")[1].split("```")[0].strip()
        elif "```" in text_content:
            text_content = text_content.split("```")[1].split("```")[0].strip()
            
        result = parser.parse(text_content)
        posteriors = result.posteriors
        leading = result.leading_hypothesis_id
        confidence = result.leader_confidence
        converged = result.converged
        
        # 3. Extract Thought Signature (from extras in text part)
        new_sig = None
        if isinstance(msg.content, list):
            for part in msg.content:
                if isinstance(part, dict):
                    extras = part.get('extras', {})
                    if 'signature' in extras:
                        new_sig = extras['signature']
                        break
        
        if new_sig:
            logger.debug("Captured thought signature from update_beliefs")
        
        logger.debug(f"âœ… Beliefs updated. Leader confidence: {confidence:.2f} ({result.reasoning[:50]}...)")
        
    except Exception as e:
        logger.error(f"Failed to update beliefs: {e}")
        return {"converged": False}

    
    # Update hypothesis confidences
    for h in hypotheses:
        if h.hypothesis_id in posteriors:
            h.current_confidence = posteriors[h.hypothesis_id]
            h.last_updated = datetime.now()
    
    # Create belief state
    belief_state = BeliefState(
        signal_id=state["signal_id"],
        signal_description=state["signal_description"],
        hypotheses=hypotheses,
        posterior_probabilities=posteriors,
        leading_hypothesis_id=leading,
        confidence_in_leader=confidence,
        converged=converged,
    )
    
    logger.info(f"âœ… Beliefs updated. Leader confidence: {confidence:.2f}")
    
    return {
        "belief_state": belief_state,
        "converged": converged,
        "iteration": state.get("iteration", 0) + 1,
        "thought_signature": new_sig
    }


async def select_action_node(state: HypothesisMarketState) -> Dict[str, Any]:
    """
    Select action based on belief state and decision policy.
    
    Uses the active reasoning artifact to score hypotheses
    and select the optimal action.
    """
    logger.info("ðŸŽ¯ Selecting action")
    
    belief_state = state.get("belief_state")
    if not belief_state:
        return {"selected_action": None}
    
    leading = belief_state.get_leading_hypothesis()
    if not leading:
        return {"selected_action": None}
    
    # Invoke Master Orchestrator as Final Judge
    from app.agents.orchestrator.orchestrator import MasterOrchestrator
    orchestrator = MasterOrchestrator()
    
    logger.info("âš–ï¸ Invoking Master Orchestrator for final judgment")
    verdict = await orchestrator.make_final_decision(belief_state)
    
    action = verdict.get("selected_action")
    reasoning = verdict.get("reasoning")
    
    logger.info(f"ðŸ‘¨â€âš–ï¸ Orchestrator Verdict: {action} ({reasoning[:50]}...)")
    
    # Check if we need human escalation based on Orchestrator's verdict
    if action == "ESCALATE_TO_HUMAN":
        return {
            "needs_human": True,
            "selected_action": "ESCALATE_TO_HUMAN",
        }
    
    return {
        "selected_action": action,
        "needs_human": False,
    }


async def execute_action_node(state: HypothesisMarketState) -> Dict[str, Any]:
    """
    Execute the selected action via appropriate agent.
    
    Routes action to the target agent specified in the
    leading hypothesis.
    """
    logger.info(f"âš¡ Executing action: {state.get('selected_action')}")
    
    action = state.get("selected_action")
    if not action or action == "ESCALATE_TO_HUMAN":
        return {"action_result": None}
    
    # Simulate action execution -> REAL EXECUTION
    # We parse the action string and call the appropriate tool
    import re
    result = {
        "success": False,
        "action": action,
        "executed_at": datetime.now().isoformat(),
        "outcome": "Action not recognized by execution node",
    }
    
    try:
        # 1. Dispatch Maintenance Crew
        # Format: dispatch_maintenance_crew(machine_id=8)
        if "dispatch_maintenance_crew" in action:
            match = re.search(r'machine_id=(\d+)', action)
            if match:
                # Call simulation service directly to avoid tool invocation complexity
                from app.services.simulation import simulation
                machine_id = int(match.group(1))
                success = simulation.dispatch_maintenance_crew(machine_id)
                
                outcome = f"SUCCESS: Maintenance Crew dispatched to Line {machine_id}" if success else f"FAILURE: Crew busy or invalid line {machine_id}"
                
                result = {
                    "success": success,
                    "action": action,
                    "executed_at": datetime.now().isoformat(),
                    "outcome": outcome
                }
        
        # 2. Schedule Maintenance
        elif "schedule_maintenance" in action:
            match = re.search(r'line_id=(\d+)', action) or re.search(r'machine_id=(\d+)', action)
            if match:
                 # Just simulate success for scheduling
                 line_id = int(match.group(1))
                 result = {
                    "success": True,
                    "action": action,
                    "executed_at": datetime.now().isoformat(),
                    "outcome": f"Maintenance scheduled for Line {line_id} (Simulated)"
                 }

        # 3. Create Work Order
        elif "create_work_order" in action:
             # Basic mock
             result = {
                 "success": True,
                 "action": action,
                 "executed_at": datetime.now().isoformat(),
                 "outcome": "Work order created (simulated)"
             }
             
        # 4. Check Fatigue / Optimize Roster (Orchestrator Action)
        elif "check fatigue" in action.lower() or "optimize roster" in action.lower():
             # Dispatch supervisor to check a central point as a "roster check"
             from app.services.simulation import simulation
             # Send to middle of floor
             simulation.dispatch_supervisor_to_location(600, 250, "Fatigue Inspection")
             result = {
                 "success": True,
                 "action": action,
                 "executed_at": datetime.now().isoformat(),
                 "outcome": "Supervisor dispatched for roster optimization check"
             }

        # 5. Critical: Evacuate (Fire/Life Safety)
        elif "evacuate" in action.lower():
             from app.services.simulation import simulation
             await simulation.trigger_evacuation()
             
             logger.critical("ðŸš¨ ACTION EXECUTED: Emergency Evacuation Triggered")
             
             result = {
                 "success": True,
                 "action": action,
                 "executed_at": datetime.now().isoformat(),
                 "outcome": "Emergency Evacuation Protocol Initiated. All staff moving to Assembly Point."
             }

        # 6. Suspend Lines (Operational Safety)
        elif "suspend line" in action.lower() or "stop line" in action.lower():
             from app.services.simulation import simulation
             # Stop all lines (without moving people)
             for line_id in simulation.machine_production:
                 await simulation._suspend_production_line(str(line_id), "Orchestrator Suspend Command")
             
             logger.warning("ðŸ›‘ ACTION EXECUTED: Production Suspended")
             
             result = {
                 "success": True,
                 "action": action,
                 "executed_at": datetime.now().isoformat(),
                 "outcome": "Production lines suspended active operations."
             }

        # Fallback for unhandled actions
        if not result.get("success") and result["outcome"] == "Action not recognized by execution node":
             logger.warning(f"âš ï¸ Action '{action}' not bound in execute_nodes.py - Simulating success")
             result["success"] = True
             result["outcome"] = "Simulated success (tool binding missing)"
             
    except Exception as e:
        logger.error(f"Failed to execute action '{action}': {e}")
        
        # Demo-friendly error handling: Graceful degradation instead of crashing
        # Show a user-friendly message and simulate escalation to human
        result = {
            "success": False,
            "action": action,
            "executed_at": datetime.now().isoformat(),
            "outcome": "Action encountered an issue - escalating to human supervisor",
            "error_detail": str(e),  # Store for debugging but don't show prominently
        }
        
        # Broadcast graceful failure to frontend
        from app.services.websocket import manager
        await manager.broadcast({
            "type": "agent_action",
            "data": {
                "agent": "orchestrator",
                "actions": [f"âš ï¸ Escalating '{action[:30]}...' to human due to system issue"],
                "timestamp": datetime.now().isoformat()
            }
        })
    
    logger.info(f"âœ… Action executed: {result['outcome']}")
    
    # Broadcast SUCCESSFUL action to frontend
    try:
        from app.services.websocket import manager
        await manager.broadcast({
            "type": "agent_action",
            "data": {
                "agent": "ORCHESTRATOR", # Action node runs under orchestrator authority
                "actions": [f"âœ… Executed: {result['action']}", f"Result: {result['outcome']}"],
                "timestamp": datetime.now().isoformat()
            }
        })
    except Exception as e:
        logger.warning(f"Failed to broadcast action: {e}")
    
    return {"action_result": result}


async def counterfactual_replay_node(state: HypothesisMarketState) -> Dict[str, Any]:
    """
    Perform counterfactual analysis after action execution.
    
    Asks: "What if we had chosen the second-best hypothesis?"
    """
    logger.info("ðŸ”„ Running counterfactual replay")
    
    belief_state = state.get("belief_state")
    if not belief_state:
        return {}
    
    chosen = belief_state.get_leading_hypothesis()
    alternative = belief_state.get_second_best()
    
    if not chosen or not alternative:
        return {}
    
    # Use Gemini to predict alternative outcome
    llm = ChatGoogleGenerativeAI(
        model=settings.gemini_model,
        google_api_key=settings.google_api_key,
    )
    
    prompt = f"""
Counterfactual analysis for manufacturing decision.

WHAT WE DID:
- Hypothesis: {chosen.description}
- Action: {state.get('selected_action')}
- Outcome: {state.get('action_result')}

ALTERNATIVE WE DIDN'T CHOOSE:
- Hypothesis: {alternative.description}
- Would have taken: {alternative.recommended_action}

Analyze:
1. What would have happened if we chose the alternative?
2. Production delta (units saved/lost)?
3. Time delta (minutes faster/slower)?
4. Risk delta (higher/lower)?
5. Strategic insight for future decisions?

Output JSON with these fields.
"""

    result = await llm.ainvoke(prompt)
    
    # Extract insight from LLM response
    insight_text = "Counterfactual analysis completed"
    try:
        import json
        import re
        content = result.content if hasattr(result, 'content') else str(result)
        json_match = re.search(r'\{.*\}', content, re.DOTALL)
        if json_match:
            parsed = json.loads(json_match.group())
            insight_text = parsed.get("insight", parsed.get("strategic_insight", insight_text))
    except Exception:
        pass
    
    # Create counterfactual replay
    replay = CounterfactualReplay(
        incident_id=state["signal_id"],
        chosen_hypothesis_id=chosen.hypothesis_id,
        chosen_hypothesis_description=chosen.description,
        action_taken=state.get("selected_action", ""),
        actual_outcome=state.get("action_result", {}),
        alternative_hypothesis_id=alternative.hypothesis_id,
        alternative_hypothesis_description=alternative.description,
        alternative_action=alternative.recommended_action or "",
        insight=insight_text,
    )
    
    # Store in strategic memory (persistent)
    await strategic_memory.add_replay(replay)
    
    logger.info(f"âœ… Counterfactual replay completed. Insight: {insight_text[:50]}...")
    
    return {"counterfactual": replay}


async def check_drift_node(state: HypothesisMarketState) -> Dict[str, Any]:
    """
    Check for framework drift and trigger rebalancing if needed.
    """
    logger.info("ðŸ“Š Checking framework drift")
    
    drift = drift_detector.detect_drift()
    
    if drift:
        logger.warning(f"âš ï¸ Drift detected: {drift.drift_type} on {drift.framework}")
    
    return {"drift_alert": drift}


async def evolve_policy_node(state: HypothesisMarketState) -> Dict[str, Any]:
    """
    Check if policy should evolve based on accumulated learning.
    
    Triggers policy update when counterfactual analysis suggests
    systematic improvements are possible.
    """
    logger.info("ðŸ“ˆ Checking policy evolution")
    
    candidates = await strategic_memory.get_policy_update_candidates()
    stats = await strategic_memory.get_stats()
    
    # Check if we should evolve
    from app.reasoning.evolver import PolicyEvolver
    evolver = PolicyEvolver(evolution_threshold=5) # Lower threshold for demo
    
    # Create dummy current policy if not in state (mocking for now since we don't persist policy in state yet)
    current_policy = DecisionPolicy(
        version="v1.0",
        confidence_threshold_act=0.7,
        confidence_threshold_escalate=0.4,
        framework_weights={"RCA": 0.3, "TOC": 0.3, "FMEA": 0.2, "COUNTERFACTUAL": 0.2}
    )
    
    # We use the evolver to check logic, but force it if we have candidates for the demo
    should_evolve = await evolver.should_evolve(current_policy, strategic_memory)
    
    if should_evolve or len(candidates) >= 3: # Aggressive evolution for demo
        logger.info(f"ðŸ”„ Policy evolution triggered! {len(candidates)} candidates")
        
        # 1. EVOLVE
        new_policy = await evolver.evolve_policy(current_policy, strategic_memory)
        
        # 2. SAVE (Extracting meta-data from the evolution result)
        # Note: evolver.evolve_policy returns the object, but we want to capture the *reasons*
        # In a real impl, evolve_policy should return a rich result. 
        # For now, we'll infer description/changes from the new policy diff
        
        changes = []
        if new_policy.confidence_threshold_act != current_policy.confidence_threshold_act:
            changes.append(f"Adjusted action threshold: {current_policy.confidence_threshold_act} -> {new_policy.confidence_threshold_act}")
        if new_policy.confidence_threshold_escalate != current_policy.confidence_threshold_escalate:
            changes.append(f"Adjusted escalation threshold: {current_policy.confidence_threshold_escalate} -> {new_policy.confidence_threshold_escalate}")
        
        # Add insights as changes
        new_insights = [i for i in new_policy.policy_insights if i not in current_policy.policy_insights]
        changes.extend(new_insights)
        
        # Save to DB
        await strategic_memory.save_policy_evolution(
            version=new_policy.version,
            confidence_threshold_act=new_policy.confidence_threshold_act,
            confidence_threshold_escalate=new_policy.confidence_threshold_escalate,
            framework_weights=new_policy.framework_weights,
            policy_insights=new_policy.policy_insights,
            incidents_evaluated=len(await strategic_memory.get_all_replays()),
            accuracy_rate=stats['accuracy_rate'],
            description=f"Strategic adjustment based on {len(candidates)} counterfactuals",
            trigger_event="Cumulative suboptimal decision patterns triggered evolution",
            changes=changes
        )
        
        # Broadcast learning event to frontend
        from app.services.websocket import manager
        await manager.broadcast({
            "type": "learning_event",
            "data": {
                "event": "policy_evolution_triggered",
                "version": new_policy.version,
                "changes": changes,
                "timestamp": datetime.now().isoformat()
            }
        })
        
        return {"policy_update_recommended": True}
    
    return {"policy_update_recommended": False}
